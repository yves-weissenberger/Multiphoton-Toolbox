{
    "docs": [
        {
            "location": "/", 
            "text": "twoptb - Two photon tool-box\n\n\nThis is a toolbox for the analysis of two-photon imaging data. It is optimised for the analysis of two-photon calcium imaging data. \n\n\nThe toolbox is centred around efficient data handling via linking of data to hdf5 files. This enables access to diverse aspects of the data and underlies in large part the extensibility of the toolbox. \n\n\nThe main strength of this toolbox, as compared to the many that already exist, are several powerful tools for data visualisation.\n\n\nA link to the repository may be found \n here \n\n\nContributors\n\n\n\n    \n \n        \n Samuel Picard", 
            "title": "Home"
        }, 
        {
            "location": "/#twoptb-two-photon-tool-box", 
            "text": "This is a toolbox for the analysis of two-photon imaging data. It is optimised for the analysis of two-photon calcium imaging data.   The toolbox is centred around efficient data handling via linking of data to hdf5 files. This enables access to diverse aspects of the data and underlies in large part the extensibility of the toolbox.   The main strength of this toolbox, as compared to the many that already exist, are several powerful tools for data visualisation.  A link to the repository may be found   here", 
            "title": "twoptb - Two photon tool-box"
        }, 
        {
            "location": "/#contributors", 
            "text": "Samuel Picard", 
            "title": "Contributors"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nMain package\n\n\nThe package has many dependencies so by far the easiest way to install the package is to download \n Anaconda 2.7\n. Thereafter only two additional packages need to be installed. These are:\n\n\n tifffile \n\n\n\n pyqtgraph \n\n\n\nThe latter from \n here \n cross plattform. For unix (mac+linux) users, the former is easily obtained by simply running \n\n\npip install tifffile\n\n\n\nwhile for windows, it can be downloaded from \n here \n and installed by running:\n\n\npip install /path/to/tifffile\u20112018.5.10\u2011cp27\u2011cp27m\u2011win_amd64.whl\n\n\n\nassuming you are running a 64 bit version of windows.\n\n\nThereafter simply download the repository from \n here \n and install it by running\n\n\npip install .\n\n\n\nin the first twoptb directory (i.e. the one containing setup.py).\n\n\nSpike Extraction\n\n\nThe spike extraction algorithm currently used is \n c2s \n. For windows users hopes for installing this package are slim at best while for unix users the installation process seems to work fine.", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#main-package", 
            "text": "The package has many dependencies so by far the easiest way to install the package is to download   Anaconda 2.7 . Thereafter only two additional packages need to be installed. These are:   tifffile    pyqtgraph   The latter from   here   cross plattform. For unix (mac+linux) users, the former is easily obtained by simply running   pip install tifffile  while for windows, it can be downloaded from   here   and installed by running:  pip install /path/to/tifffile\u20112018.5.10\u2011cp27\u2011cp27m\u2011win_amd64.whl  assuming you are running a 64 bit version of windows.  Thereafter simply download the repository from   here   and install it by running  pip install .  in the first twoptb directory (i.e. the one containing setup.py).", 
            "title": "Main package"
        }, 
        {
            "location": "/install/#spike-extraction", 
            "text": "The spike extraction algorithm currently used is   c2s  . For windows users hopes for installing this package are slim at best while for unix users the installation process seems to work fine.", 
            "title": "Spike Extraction"
        }, 
        {
            "location": "/overview/", 
            "text": "Overview\n\n\nThis page is intended to provide a quick overview of the workflow. For more information please see the \nUser Guide\n\n\nTypical Set of function calls to preprocess data\n\n\nMost important functions have (will soon have) help. To view the help run the function simply as:\n\n\nFirst format data by calling \n\n\nconvert_to_hdf5.py path/to/datafolder/\n\n\n\nNext motion register data\n\n\nmotion_register_data.py path/to/hdf5.h5\n\n\n\nThen draw ROIs either manually\n\n\nROI_Drawer.py path/to/hdf5.h5\n\n\n\nor automatically\n\n\nrun_roi_finder.py finder_name path/to/hdf5.h5\n\n\n\nfollowed by manul curation using the \"ROI_Drawer\"\n\n\nThen, if the data contains several separate acquisition runs involving the same cells\n\n\nshare_roiinfo.py path/to/hdf5.h5\n\n\n\nFinally extract traces from the cells:\n\n\nextract_roi_traces.py path/to/hdf5.h5\n\n\n\nGeneral Advice\n\n\nThe key functions defining the pipeline are run by calling (via command line)\n\n\npython /path/to/twoptb/twoptb/scripts/generic_script.py -h", 
            "title": "Quickstart"
        }, 
        {
            "location": "/overview/#overview", 
            "text": "This page is intended to provide a quick overview of the workflow. For more information please see the  User Guide", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#typical-set-of-function-calls-to-preprocess-data", 
            "text": "Most important functions have (will soon have) help. To view the help run the function simply as:  First format data by calling   convert_to_hdf5.py path/to/datafolder/  Next motion register data  motion_register_data.py path/to/hdf5.h5  Then draw ROIs either manually  ROI_Drawer.py path/to/hdf5.h5  or automatically  run_roi_finder.py finder_name path/to/hdf5.h5  followed by manul curation using the \"ROI_Drawer\"  Then, if the data contains several separate acquisition runs involving the same cells  share_roiinfo.py path/to/hdf5.h5  Finally extract traces from the cells:  extract_roi_traces.py path/to/hdf5.h5", 
            "title": "Typical Set of function calls to preprocess data"
        }, 
        {
            "location": "/overview/#general-advice", 
            "text": "The key functions defining the pipeline are run by calling (via command line)  python /path/to/twoptb/twoptb/scripts/generic_script.py -h", 
            "title": "General Advice"
        }, 
        {
            "location": "/user_guide/data_conversion/", 
            "text": "Data conversion\n\n\nPrior to any preprocessing or analysis of the raw data, it is converted from the raw data format, (i.e. .tif) to HDF5 for convenience. The resulting HDF file serves as a central access point for the multimodal (i.e. two-photon imaging data, video-data, stimulus files etc.).  \n\n\nData is converted to HDF5 by running, in terminal \n\n\nconvert_to_hdf5.py /path/to/data_folder/\n\n\n\nIn order to preprocess data properly, the script requires a certain directory and file structure.  Firstly, it assumes that imaging data are stored in the form of .tif files with the metadata required for reading them. Additionally, it assumes that these .tif files are stored in directories containing GRABinfo files (which contain image acquisition parameters) and optionally stimulus script files (outDat.mat file in the example, see below for further details). Finally, it assumes that data are in a certain directory structure: \n\n\nExpects certain folder structure e.g.\n/home\n/AIAK_2\n    /29082017\n        /session01\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...\n\n        /session02\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...\n\n        /session03\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...\n\n\n\nIn the case of the above directory structure, the script above would be run as\n\n\nconvert_to_hdf5.py /home/AIAK_2/\n\n\n\nand would create a single HDF5 file with data from all acquired sessions. Importantly, to optimally use this toolbox, all sessions in the example above should either be acquisitions of the same group of cells, or should all be acquisitions of different groups of cells. \nRunning this command would create a single HDF5 file from which raw-data, acquisition parameters and stimulus data could be convienetly and reproducibly accessed. \n\n\nLinking stimulus scripts\n\n\nA core feature of the toolbox is the centrality of the hdf5 file as a link for the multimodal data. For the analysis of most imaging datasets, additional information is required. This can be of the form of behavioural timestamps, simultaneously recorded video (currently not supported), or stimulus timestamps. Integrating this timestamped data with the HDF5 file requires processing of the raw form of the timestamp data. In order to link custom scripts to the HDF5 file, a readout script must be added to the \"load_events.py\" file which can be found in \n\n\ntwoptb/twoptb/file_management/load_events.py\n\n\n\nAfter adding the appropriate function, the data must be directed to this function by adding the appropriate path to\n\n\ntwoptb/twoptb/file_management/load_images.py\n\n\n\nFor example, to extract data from the ouDat.mat file in the example directory structure above \"load_images.py\" contains the following elif statement \n\n\n    elif 'outDat' in fname:\n        if '._' not in fname:\n            matFilePth = os.path.join(directory,fname)\n            print matFilePth\n            stimattrs = get_triggers(matFilePth)\n\n\n\nwhich directs processing of the outDat file to the function \"get_triggers\" which can be found in load_events.py.\n\n\nOutput Folder Structure\n\n\nRunning the above script should give rise to the above directory structure\n    /home\n    /AIAK_2\n        /29082017\n            /...\n        /processed\n            /29082017_AIAK_2\n                29082017_AIAK_2.h5\n                /GRABinfos\n                /stims\n                /regInfo\n                /ROIs\n\n\n    proc_log.txt\n\n\n\nAfter this initial preprocessing step next is \nmotion registration", 
            "title": "Data Conversion"
        }, 
        {
            "location": "/user_guide/data_conversion/#data-conversion", 
            "text": "Prior to any preprocessing or analysis of the raw data, it is converted from the raw data format, (i.e. .tif) to HDF5 for convenience. The resulting HDF file serves as a central access point for the multimodal (i.e. two-photon imaging data, video-data, stimulus files etc.).    Data is converted to HDF5 by running, in terminal   convert_to_hdf5.py /path/to/data_folder/  In order to preprocess data properly, the script requires a certain directory and file structure.  Firstly, it assumes that imaging data are stored in the form of .tif files with the metadata required for reading them. Additionally, it assumes that these .tif files are stored in directories containing GRABinfo files (which contain image acquisition parameters) and optionally stimulus script files (outDat.mat file in the example, see below for further details). Finally, it assumes that data are in a certain directory structure:   Expects certain folder structure e.g.\n/home\n/AIAK_2\n    /29082017\n        /session01\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...\n\n        /session02\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...\n\n        /session03\n            /Acq1.tif\n            GRABinfo.mat\n            outDat.mat\n            ...  In the case of the above directory structure, the script above would be run as  convert_to_hdf5.py /home/AIAK_2/  and would create a single HDF5 file with data from all acquired sessions. Importantly, to optimally use this toolbox, all sessions in the example above should either be acquisitions of the same group of cells, or should all be acquisitions of different groups of cells. \nRunning this command would create a single HDF5 file from which raw-data, acquisition parameters and stimulus data could be convienetly and reproducibly accessed.", 
            "title": "Data conversion"
        }, 
        {
            "location": "/user_guide/data_conversion/#linking-stimulus-scripts", 
            "text": "A core feature of the toolbox is the centrality of the hdf5 file as a link for the multimodal data. For the analysis of most imaging datasets, additional information is required. This can be of the form of behavioural timestamps, simultaneously recorded video (currently not supported), or stimulus timestamps. Integrating this timestamped data with the HDF5 file requires processing of the raw form of the timestamp data. In order to link custom scripts to the HDF5 file, a readout script must be added to the \"load_events.py\" file which can be found in   twoptb/twoptb/file_management/load_events.py  After adding the appropriate function, the data must be directed to this function by adding the appropriate path to  twoptb/twoptb/file_management/load_images.py  For example, to extract data from the ouDat.mat file in the example directory structure above \"load_images.py\" contains the following elif statement       elif 'outDat' in fname:\n        if '._' not in fname:\n            matFilePth = os.path.join(directory,fname)\n            print matFilePth\n            stimattrs = get_triggers(matFilePth)  which directs processing of the outDat file to the function \"get_triggers\" which can be found in load_events.py.", 
            "title": "Linking stimulus scripts"
        }, 
        {
            "location": "/user_guide/data_conversion/#output-folder-structure", 
            "text": "Running the above script should give rise to the above directory structure\n    /home\n    /AIAK_2\n        /29082017\n            /...\n        /processed\n            /29082017_AIAK_2\n                29082017_AIAK_2.h5\n                /GRABinfos\n                /stims\n                /regInfo\n                /ROIs      proc_log.txt  After this initial preprocessing step next is  motion registration", 
            "title": "Output Folder Structure"
        }, 
        {
            "location": "/user_guide/motionreg/", 
            "text": "Motion registration\n\n\nTo run motion registration on a dataset that has been converted to hdf5, simply run\n\n\nmotion_register_data.py path/to/hdf_file.h5\n\n\n\nMotion Registration Algorithm\n\n\nMotion registration is implemented using the efficient subpixel registration algorithm provided by Guizmar. \n\n\nMean image selection\n\n\nMean images are computed by", 
            "title": "Motion Registration"
        }, 
        {
            "location": "/user_guide/motionreg/#motion-registration", 
            "text": "To run motion registration on a dataset that has been converted to hdf5, simply run  motion_register_data.py path/to/hdf_file.h5", 
            "title": "Motion registration"
        }, 
        {
            "location": "/user_guide/motionreg/#motion-registration-algorithm", 
            "text": "Motion registration is implemented using the efficient subpixel registration algorithm provided by Guizmar.", 
            "title": "Motion Registration Algorithm"
        }, 
        {
            "location": "/user_guide/motionreg/#mean-image-selection", 
            "text": "Mean images are computed by", 
            "title": "Mean image selection"
        }, 
        {
            "location": "/user_guide/rois/", 
            "text": "ROI Definition Methods\n\n\nThere are two approaches to ROI definition that can be used in the context of this toolbox and they are by and large complementary. The first is an \nautomatic algorithm\n; the second is a \nManual Curation GUI\n\n\nAutomatic Approach\n\n\n\nThe automatic approach to ROI definition is a two-step process. In the first step, putative centroids of ROIs are identified. This is done by moving a sliding window \n\n\nTraining an ROI classifer\n\n\nA new ROI classifier can be trained (specifying all optional arguments here) by running the following command    \n\n\ntrain_roi_finder.py -rad 7 -shifts 3 7 ai93_zoom1 /path/to/folder\n\n\n\nThere are two requried arguments, which will be addressed first. The first, in this case \"ai93_zoom1\" is the name to be given to the roi_finder algorithm that is being trained.\nThe second argument, \"/path/to/folder\" is a path to a folder containing one or more hdf5 files for which ROIs have already been defined, to be used as a training set. Importantly, multiple folders can be specified if this is appropriate:\n\n\ntrain_roi_finder.py -rad 7 -shifts 3 7 ai93_zoom1 /path/to/folder1 /path/to/folder2\n\n\n\nthe rad argument specifies the size of the image patch that will be used to train the ROI_finder on, in pixels. \nShifts specifies the size of shifts away from previously\n\n\nUsing a pre-trained ROI classifier\n\n\nWe have developed a simple algorithm for automatic roi definition. To run this algorithm, run:\n\n\nrun_roi_finder.py -sess -1 -ded 3 2 2 -thresh 0.96 roi_finder_name /path/to/hdf5.h5\n\n\n\nwhere roi_finder name is an automatic roi finder derived from training.\n\n\nBelow is an example of a 1mm patch of cortex in which automatic roi segmentation has been run\n\n\n \n\n\nPhysiological Validation\n\n\nA potential concern is that our automatic method relies soley on the mean image for identification. This may bias cell selection towards those that are uninteresting. We therefore validated the approach by comparing topographic organisation estimated using ROIs selected manually based on both mean image and full image stack (acquired by \nManual Curation GUI\n) and those based purely on automatic ROI drawing. Included in the images are those ROIs which, after a simple ANOVA, are deemed significantly frequency tuned (p\n0.01). \n\n\n\n\nData and and analysis run by  \n Samuel Picard \n\n\nManual Curation\n\n\n\nTo open the GUI for manual curation of ROIs run\n\n\nROI_Drawer.py /path/to/hdf5.h5\n\n\n\nThis tool can be run with two optional arguments the first being whether to extract traces as ROIs are drawn (default is false). To enable rapid drawing run\n\n\nROI_Drawer.py -o /path/to/hdf5.h5\n\n\n\nSecondly, and analogously, adding the -restart flag starts ROI drawing for this hdf file from the beginning, deleting previous work\n\n\nwhich should open a dialog asking which session you wish to process:\n\n\n \n\n\nControls\n\n\nSharing ROIs across sessions \n\n\n\nIn many cases, multiple acquisitions run on the same day will be of the same field of view. In this case, it is desirable to share ROIs across those sessions. To do this, after drawing ROIs for one session using manual curation, run\n\n\nshare_roi_info.py /path/to/hdf5.h5\n\n\n\nafter drawing \n EACH \n session. Running it after drawing the ROIs for each session is crucial here as \n running this code block will overwrite ROIs drawn which belong to other sessions.", 
            "title": "ROI Definition"
        }, 
        {
            "location": "/user_guide/rois/#roi-definition-methods", 
            "text": "There are two approaches to ROI definition that can be used in the context of this toolbox and they are by and large complementary. The first is an  automatic algorithm ; the second is a  Manual Curation GUI", 
            "title": "ROI Definition Methods"
        }, 
        {
            "location": "/user_guide/rois/#training-an-roi-classifer", 
            "text": "A new ROI classifier can be trained (specifying all optional arguments here) by running the following command      train_roi_finder.py -rad 7 -shifts 3 7 ai93_zoom1 /path/to/folder  There are two requried arguments, which will be addressed first. The first, in this case \"ai93_zoom1\" is the name to be given to the roi_finder algorithm that is being trained.\nThe second argument, \"/path/to/folder\" is a path to a folder containing one or more hdf5 files for which ROIs have already been defined, to be used as a training set. Importantly, multiple folders can be specified if this is appropriate:  train_roi_finder.py -rad 7 -shifts 3 7 ai93_zoom1 /path/to/folder1 /path/to/folder2  the rad argument specifies the size of the image patch that will be used to train the ROI_finder on, in pixels. \nShifts specifies the size of shifts away from previously", 
            "title": "Training an ROI classifer"
        }, 
        {
            "location": "/user_guide/rois/#using-a-pre-trained-roi-classifier", 
            "text": "We have developed a simple algorithm for automatic roi definition. To run this algorithm, run:  run_roi_finder.py -sess -1 -ded 3 2 2 -thresh 0.96 roi_finder_name /path/to/hdf5.h5  where roi_finder name is an automatic roi finder derived from training.  Below is an example of a 1mm patch of cortex in which automatic roi segmentation has been run", 
            "title": "Using a pre-trained ROI classifier"
        }, 
        {
            "location": "/user_guide/rois/#physiological-validation", 
            "text": "A potential concern is that our automatic method relies soley on the mean image for identification. This may bias cell selection towards those that are uninteresting. We therefore validated the approach by comparing topographic organisation estimated using ROIs selected manually based on both mean image and full image stack (acquired by  Manual Curation GUI ) and those based purely on automatic ROI drawing. Included in the images are those ROIs which, after a simple ANOVA, are deemed significantly frequency tuned (p 0.01).    Data and and analysis run by    Samuel Picard", 
            "title": "Physiological Validation"
        }, 
        {
            "location": "/user_guide/rois/#controls", 
            "text": "", 
            "title": "Controls"
        }, 
        {
            "location": "/user_guide/trace_extraction/", 
            "text": "", 
            "title": "Trace Extraction"
        }, 
        {
            "location": "/user_guide/across_days/", 
            "text": "Matching ROIs across days\n\n\nIf the same group of cells is repeatedly imaged across days, many experiments require matching cells across days. To enable this, \n\n\nAlgorithmic Approach\n\n\nThe automatic approach works by registering local patches across the image day by day. Once you have ROI masks across the same imaging field of view (FOV) across multiple days, the ROI alignment can be performed by calling:\n\n\naggregate_rois.py /path/to/folder\n\n\n\nwhere /path/to/folder is the path to folder that is parent to the hdf5 files with ROIs that should be algigned. Importantly, the folder must not contain other files. If there is no such parent folder, the code can be run by calling:\n\n\naggregate_rois.py /path/to/folder1 /path/to/folder2\n\n\n\noptionally as many folders as desired\n\n\nrunning this script creates an additional pickle file in each hdf5 directory\n\n\n\n    /home\n        /AIAK_2\n            /29082017\n                /...\n            /processed\n                /29082017_AIAK_2\n                    29082017_AIAK_2.h5\n                    \n29082017_AIAK_2_ROIS_glob.p \n\n                    /GRABinfos\n                    /stims\n                    /regInfo\n                    /ROIs\n\n            proc_log.txt\n\n\n\n\n\n\nAcross the ROIs_glob files belong each day, the index of each ROI is kept constant.\n\n\nGUI for curation\n\n\nIn order to open the GUI for across day ROI curation, run\n\n\nacross_day_rois.py /path/to/folder\n\n\n\nanalogously to \"aggregate_rois\" as explained above. This should open the GUI:\n\n\n\n\nControls\n\n\nUse arrow keys to cycle through ROIs.\nDouble clicking one of the images leads to its selection (indicated by the box around it turning blue). Holding down shift then allows the ROI on a particular day to be moved around.\n\n\nAn additional feature is the assignment of confidence that a given set of ROIs are the same. Confidence is assigned by pressing 1,2 or 3 and then clicking on the ROI. To assign an ROI as being absent, click on an image while holding down the CRTL key. \nFinally, if a given day has multiple sessions, the mean images from each session can be cycled through by selecing a given ROI (by double clicking) and then moving through images by pressing the up and down arrow keys", 
            "title": "Across Day Analysis"
        }, 
        {
            "location": "/user_guide/across_days/#matching-rois-across-days", 
            "text": "If the same group of cells is repeatedly imaged across days, many experiments require matching cells across days. To enable this,", 
            "title": "Matching ROIs across days"
        }, 
        {
            "location": "/user_guide/across_days/#algorithmic-approach", 
            "text": "The automatic approach works by registering local patches across the image day by day. Once you have ROI masks across the same imaging field of view (FOV) across multiple days, the ROI alignment can be performed by calling:  aggregate_rois.py /path/to/folder  where /path/to/folder is the path to folder that is parent to the hdf5 files with ROIs that should be algigned. Importantly, the folder must not contain other files. If there is no such parent folder, the code can be run by calling:  aggregate_rois.py /path/to/folder1 /path/to/folder2  optionally as many folders as desired  running this script creates an additional pickle file in each hdf5 directory  \n    /home\n        /AIAK_2\n            /29082017\n                /...\n            /processed\n                /29082017_AIAK_2\n                    29082017_AIAK_2.h5\n                     29082017_AIAK_2_ROIS_glob.p  \n                    /GRABinfos\n                    /stims\n                    /regInfo\n                    /ROIs\n\n            proc_log.txt   Across the ROIs_glob files belong each day, the index of each ROI is kept constant.", 
            "title": "Algorithmic Approach"
        }, 
        {
            "location": "/user_guide/across_days/#gui-for-curation", 
            "text": "In order to open the GUI for across day ROI curation, run  across_day_rois.py /path/to/folder  analogously to \"aggregate_rois\" as explained above. This should open the GUI:", 
            "title": "GUI for curation"
        }, 
        {
            "location": "/user_guide/across_days/#controls", 
            "text": "Use arrow keys to cycle through ROIs.\nDouble clicking one of the images leads to its selection (indicated by the box around it turning blue). Holding down shift then allows the ROI on a particular day to be moved around.  An additional feature is the assignment of confidence that a given set of ROIs are the same. Confidence is assigned by pressing 1,2 or 3 and then clicking on the ROI. To assign an ROI as being absent, click on an image while holding down the CRTL key. \nFinally, if a given day has multiple sessions, the mean images from each session can be cycled through by selecing a given ROI (by double clicking) and then moving through images by pressing the up and down arrow keys", 
            "title": "Controls"
        }
    ]
}